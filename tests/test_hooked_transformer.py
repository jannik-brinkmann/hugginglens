import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from hugginglens.hooked_transformer import HookedHFTransformer


DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MODEL_NAME_OR_PATH = "gpt2"
PROMPT = "Hello World!"

act_names_in_cache = [
    'transformer.wte', 
    'transformer.wpe', 
    'transformer.drop', 
    'transformer.h.0.ln_1', 
    'transformer.h.0.attn.c_attn', 
    'transformer.h.0.attn.attn_dropout', 
    'transformer.h.0.attn.c_proj', 
    'transformer.h.0.attn.resid_dropout', 
    'transformer.h.0.ln_2', 
    'transformer.h.0.mlp.c_fc', 
    'transformer.h.0.mlp.act', 
    'transformer.h.0.mlp.c_proj', 
    'transformer.h.0.mlp.dropout', 
    'transformer.h.0.mlp', 
    'transformer.h.1.ln_1', 
    'transformer.h.1.attn.c_attn', 
    'transformer.h.1.attn.attn_dropout', 
    'transformer.h.1.attn.c_proj', 
    'transformer.h.1.attn.resid_dropout', 
    'transformer.h.1.ln_2', 
    'transformer.h.1.mlp.c_fc',
    'transformer.h.1.mlp.act', 
    'transformer.h.1.mlp.c_proj', 
    'transformer.h.1.mlp.dropout', 
    'transformer.h.1.mlp', 
    'transformer.h.2.ln_1', 
    'transformer.h.2.attn.c_attn', 
    'transformer.h.2.attn.attn_dropout', 
    'transformer.h.2.attn.c_proj', 
    'transformer.h.2.attn.resid_dropout', 
    'transformer.h.2.ln_2', 
    'transformer.h.2.mlp.c_fc', 
    'transformer.h.2.mlp.act', 
    'transformer.h.2.mlp.c_proj', 
    'transformer.h.2.mlp.dropout', 
    'transformer.h.2.mlp', 
    'transformer.h.3.ln_1', 
    'transformer.h.3.attn.c_attn', 
    'transformer.h.3.attn.attn_dropout', 
    'transformer.h.3.attn.c_proj', 
    'transformer.h.3.attn.resid_dropout', 
    'transformer.h.3.ln_2', 
    'transformer.h.3.mlp.c_fc', 
    'transformer.h.3.mlp.act', 
    'transformer.h.3.mlp.c_proj', 
    'transformer.h.3.mlp.dropout', 
    'transformer.h.3.mlp', 
    'transformer.h.4.ln_1', 
    'transformer.h.4.attn.c_attn', 
    'transformer.h.4.attn.attn_dropout', 
    'transformer.h.4.attn.c_proj', 
    'transformer.h.4.attn.resid_dropout', 
    'transformer.h.4.ln_2', 
    'transformer.h.4.mlp.c_fc', 
    'transformer.h.4.mlp.act', 
    'transformer.h.4.mlp.c_proj', 
    'transformer.h.4.mlp.dropout', 
    'transformer.h.4.mlp',
    'transformer.h.5.ln_1',
    'transformer.h.5.attn.c_attn', 
    'transformer.h.5.attn.attn_dropout', 
    'transformer.h.5.attn.c_proj', 
    'transformer.h.5.attn.resid_dropout', 
    'transformer.h.5.ln_2', 
    'transformer.h.5.mlp.c_fc', 
    'transformer.h.5.mlp.act', 
    'transformer.h.5.mlp.c_proj', 
    'transformer.h.5.mlp.dropout', 
    'transformer.h.5.mlp', 
    'transformer.h.6.ln_1', 
    'transformer.h.6.attn.c_attn', 
    'transformer.h.6.attn.attn_dropout', 
    'transformer.h.6.attn.c_proj', 
    'transformer.h.6.attn.resid_dropout', 
    'transformer.h.6.ln_2',
    'transformer.h.6.mlp.c_fc', 
    'transformer.h.6.mlp.act', 
    'transformer.h.6.mlp.c_proj', 
    'transformer.h.6.mlp.dropout', 
    'transformer.h.6.mlp', 
    'transformer.h.7.ln_1',
    'transformer.h.7.attn.c_attn', 
    'transformer.h.7.attn.attn_dropout', 
    'transformer.h.7.attn.c_proj', 
    'transformer.h.7.attn.resid_dropout', 
    'transformer.h.7.ln_2', 
    'transformer.h.7.mlp.c_fc', 
    'transformer.h.7.mlp.act', 
    'transformer.h.7.mlp.c_proj', 
    'transformer.h.7.mlp.dropout', 
    'transformer.h.7.mlp', 
    'transformer.h.8.ln_1', 
    'transformer.h.8.attn.c_attn', 
    'transformer.h.8.attn.attn_dropout', 
    'transformer.h.8.attn.c_proj', 
    'transformer.h.8.attn.resid_dropout', 
    'transformer.h.8.ln_2', 
    'transformer.h.8.mlp.c_fc', 
    'transformer.h.8.mlp.act', 
    'transformer.h.8.mlp.c_proj', 
    'transformer.h.8.mlp.dropout', 
    'transformer.h.8.mlp', 
    'transformer.h.9.ln_1', 
    'transformer.h.9.attn.c_attn', 
    'transformer.h.9.attn.attn_dropout', 
    'transformer.h.9.attn.c_proj', 
    'transformer.h.9.attn.resid_dropout', 
    'transformer.h.9.ln_2',
    'transformer.h.9.mlp.c_fc', 
    'transformer.h.9.mlp.act', 
    'transformer.h.9.mlp.c_proj', 
    'transformer.h.9.mlp.dropout',
    'transformer.h.9.mlp', 
    'transformer.h.10.ln_1', 
    'transformer.h.10.attn.c_attn', 
    'transformer.h.10.attn.attn_dropout', 
    'transformer.h.10.attn.c_proj', 
    'transformer.h.10.attn.resid_dropout', 
    'transformer.h.10.ln_2', 
    'transformer.h.10.mlp.c_fc', 
    'transformer.h.10.mlp.act', 
    'transformer.h.10.mlp.c_proj', 
    'transformer.h.10.mlp.dropout', 
    'transformer.h.10.mlp', 
    'transformer.h.11.ln_1', 
    'transformer.h.11.attn.c_attn',
    'transformer.h.11.attn.attn_dropout', 
    'transformer.h.11.attn.c_proj', 
    'transformer.h.11.attn.resid_dropout', 
    'transformer.h.11.ln_2', 
    'transformer.h.11.mlp.c_fc', 
    'transformer.h.11.mlp.act', 
    'transformer.h.11.mlp.c_proj', 
    'transformer.h.11.mlp.dropout', 
    'transformer.h.11.mlp', 
    'transformer.ln_f',
    'lm_head'
]


def test_cache_hook_names():
    # setup model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME_OR_PATH)
    hooked_model = HookedHFTransformer(model, DEVICE)

    # cache activations
    tokens = tokenizer([PROMPT], return_tensors="pt").to(DEVICE) 
    _, cache = hooked_model.run_with_cache(**tokens)
    assert list(cache.keys()) == act_names_in_cache
